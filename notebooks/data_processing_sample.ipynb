{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Raghu Studies\\LLAMA_FinancialAdvisor\\.src_fin_advisor\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import pandas as pd\n",
    "from unstructured.cleaners.core import clean_extra_whitespace, group_broken_paragraphs\n",
    "#json --> deserialzed --> convert to hugging face dataset --> prompt setup --> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('D:\\Raghu Studies\\LLAMA_FinancialAdvisor\\dataset_final.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'about_me': 'I am a 21 year old college student.I was thinking of investing in the stock market.',\n",
       "  'context': 'Meta fires 10k employees.\\nMeta about to release Threads app.\\nZuckerberg to visit China soon',\n",
       "  'response': 'Monitor Meta due to layoffs and app release. Wait for stability before investing. Recent layoffs and impending app release may impact stock. Wait for stability.',\n",
       "  'question': 'Is Meta a good stock to buy?'},\n",
       " {'about_me': \"I am a 28 year old marketing professional.I have some savings and I'm interested in crypto investments.\",\n",
       "  'context': \"El Salvador adopts Bitcoin as legal tender.\\nRecent fluctuations in Bitcoin's price.\\nRenewed interest from institutional investors\",\n",
       "  'response': \"Invest in Bitcoin for long-term gains. Legal tender status and institutional interest signal growth. Bitcoin's adoption as legal tender and institutional interest indicate potential growth.\",\n",
       "  'question': 'Is Bitcoin a good investment option?'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataSample:\n",
    "    \"\"\"\n",
    "    A data sample for a question answering model.\n",
    "\n",
    "    Attributes:\n",
    "        user_context (str): The user's context for the question.\n",
    "        news_context (str): The news context for the question.\n",
    "        chat_history (str): The chat history for the question.\n",
    "        question (str): The question to be answered.\n",
    "        answer (str): The answer to the question.\n",
    "    \"\"\"\n",
    "\n",
    "    user_context: str = field(repr=False)\n",
    "    news_context: str = \"\"\n",
    "    chat_history: str = \"\"\n",
    "    question: str = \"\"\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasample = [DataSample(user_context = sample['about_me'],\n",
    "            news_context = sample['context'],\n",
    "            chat_history = sample.get('chat_history',''),\n",
    "            question=sample['question'],\n",
    "            answer=sample['response']) for sample in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_as_dict = [asdict(sample) for sample in datasample]\n",
    "dataset = Dataset.from_list(data_as_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class PromptTemplate:\n",
    "    \"\"\"A class that manages prompt templates\"\"\"\n",
    "\n",
    "    name: str\n",
    "    system_template: str = \"{system_message}\"\n",
    "    context_template: str = \"{user_context}\\n{news_context}\"\n",
    "    chat_history_template: str = \"{chat_history}\"\n",
    "    question_template: str = \"{question}\"\n",
    "    answer_template: str = \"{answer}\"\n",
    "    system_message: str = \"\"\n",
    "    sep: str = \"\\n\"\n",
    "    eos: str = \"\"\n",
    "\n",
    "    @property\n",
    "    def input_variables(self) -> List[str]:\n",
    "        \"\"\"Returns a list of input variables for the prompt template\"\"\"\n",
    "        return [\"user_context\", \"news_context\", \"chat_history\", \"question\", \"answer\"]\n",
    "\n",
    "    @property\n",
    "    def train_raw_template(self):\n",
    "        \"\"\"Returns the training prompt template format\"\"\"\n",
    "\n",
    "        system = self.system_template.format(system_message=self.system_message)\n",
    "        context = f\"{self.sep}{self.context_template}\"\n",
    "        chat_history = f\"{self.sep}{self.chat_history_template}\"\n",
    "        question = f\"{self.sep}{self.question_template}\"\n",
    "        answer = f\"{self.sep}{self.answer_template}\"\n",
    "\n",
    "        return f\"{system}{context}{chat_history}{question}{answer}{self.eos}\"\n",
    "\n",
    "    @property\n",
    "    def infer_raw_template(self):\n",
    "        \"\"\"Returns the inference prompt template format\"\"\"\n",
    "\n",
    "        system = self.system_template.format(system_message=self.system_message)\n",
    "        context = f\"{self.sep}{self.context_template}\"\n",
    "        chat_history = f\"{self.sep}{self.chat_history_template}\"\n",
    "        question = f\"{self.sep}{self.question_template}\"\n",
    "\n",
    "        return f\"{system}{context}{chat_history}{question}{self.eos}\"\n",
    "\n",
    "    def format_train(self, sample: Dict[str, str]) -> Dict[str, Union[str, Dict]]:\n",
    "        \"\"\"Formats the data sample to a training sample\"\"\"\n",
    "        \n",
    "        prompt = self.train_raw_template.format(\n",
    "            user_context=sample[\"user_context\"],\n",
    "            news_context=sample[\"news_context\"],\n",
    "            chat_history=sample.get(\"chat_history\", \"\"),\n",
    "            question=sample[\"question\"],\n",
    "            answer=sample[\"answer\"],\n",
    "        )\n",
    "\n",
    "        return {\"prompt\": prompt, \"payload\": sample}\n",
    "\n",
    "    def format_infer(self, sample: Dict[str, str]) -> Dict[str, Union[str, Dict]]:\n",
    "        \"\"\"Formats the data sample to a testing sample\"\"\"\n",
    "\n",
    "        prompt = self.infer_raw_template.format(\n",
    "            user_context=sample[\"user_context\"],\n",
    "            news_context=sample[\"news_context\"],\n",
    "            chat_history=sample.get(\"chat_history\", \"\"),\n",
    "            question=sample[\"question\"],\n",
    "        )\n",
    "        return {\"prompt\": prompt, \"payload\": sample}\n",
    "\n",
    "\n",
    "# Global Templates registry\n",
    "templates: Dict[str, PromptTemplate] = {}\n",
    "\n",
    "\n",
    "def register_llm_template(template: PromptTemplate):\n",
    "    \"\"\"Register a new template to the global templates registry\"\"\"\n",
    "\n",
    "    templates[template.name] = template\n",
    "\n",
    "\n",
    "def get_llm_template(name: str) -> PromptTemplate:\n",
    "    \"\"\"Returns the template assigned to the given name\"\"\"\n",
    "\n",
    "    return templates[name]\n",
    "\n",
    "\n",
    "##### Register Templates #####\n",
    "# - FALCON (spec: https://huggingface.co/tiiuae/falcon-7b/blob/main/tokenizer.json)\n",
    "register_llm_template(\n",
    "    PromptTemplate(\n",
    "        name=\"falcon\",\n",
    "        system_template=\">>INTRODUCTION<< {system_message}\",\n",
    "        system_message=\"You are a helpful assistant, with financial expertise.\",\n",
    "        context_template=\">>DOMAIN<< {user_context}\\n{news_context}\",\n",
    "        chat_history_template=\">>SUMMARY<< {chat_history}\",\n",
    "        question_template=\">>QUESTION<< {question}\",\n",
    "        answer_template=\">>ANSWER<< {answer}\",\n",
    "        sep=\"\\n\",\n",
    "        eos=\"<|endoftext|>\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_template = get_llm_template(\"falcon\")\n",
    "template_mapping_func = _template.format_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(samples):\n",
    "    for key, sample in samples.items():\n",
    "        cleaned_sample = clean_extra_whitespace(sample)\n",
    "        cleaned_sample = group_broken_paragraphs(cleaned_sample)\n",
    "\n",
    "        samples[key] = cleaned_sample\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 84/84 [00:00<00:00, 701.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 84/84 [00:00<00:00, 558.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(template_mapping_func, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '>>INTRODUCTION<< You are a helpful assistant, with financial expertise.\\n>>DOMAIN<< I am a 21 year old college student.I was thinking of investing in the stock market.\\nMeta fires 10k employees. Meta about to release Threads app. Zuckerberg to visit China soon\\n>>SUMMARY<< \\n>>QUESTION<< Is Meta a good stock to buy?\\n>>ANSWER<< Monitor Meta due to layoffs and app release. Wait for stability before investing. Recent layoffs and impending app release may impact stock. Wait for stability.<|endoftext|>',\n",
       " 'payload': {'answer': 'Monitor Meta due to layoffs and app release. Wait for stability before investing. Recent layoffs and impending app release may impact stock. Wait for stability.',\n",
       "  'chat_history': '',\n",
       "  'news_context': 'Meta fires 10k employees. Meta about to release Threads app. Zuckerberg to visit China soon',\n",
       "  'question': 'Is Meta a good stock to buy?',\n",
       "  'user_context': 'I am a 21 year old college student.I was thinking of investing in the stock market.'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftConfig, PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'tiiuae/falcon-7b-instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "No package metadata was found for bitsandbytes",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\metadata\\__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,\n\u001b[0;32m      7\u001b[0m                                              revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                                              quantization_config \u001b[38;5;241m=\u001b[39m bnb_config,\n\u001b[0;32m      9\u001b[0m                                              load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                                              device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m                                              trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,\n\u001b[0;32m     14\u001b[0m                                           trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m                                           truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Raghu Studies\\LLAMA_FinancialAdvisor\\.src_fin_advisor\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:265\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.__init__\u001b[1;34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_compute_dtype must be a string or a torch.dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Raghu Studies\\LLAMA_FinancialAdvisor\\.src_fin_advisor\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:311\u001b[0m, in \u001b[0;36mBitsAndBytesConfig.post_init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_4bit_use_double_quant, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbnb_4bit_use_double_quant must be a boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_in_4bit \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbitsandbytes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m ):\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4 bit quantization requires bitsandbytes>=0.39.0 - please upgrade your bitsandbytes version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    316\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\metadata\\__init__.py:1008\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\metadata\\__init__.py:981\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    976\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m \n\u001b[0;32m    978\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\metadata\\__init__.py:565\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: No package metadata was found for bitsandbytes"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_compute_dtype=torch.float16,\n",
    "                                bnb_4bit_quant_type='bitsandbytes',\n",
    "                                bnb_4bit_use_double_quant=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             revision='main',\n",
    "                                             quantization_config = bnb_config,\n",
    "                                             load_in_4bit=True,\n",
    "                                             device_map='auto',\n",
    "                                             trust_remote_code=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                          trust_remote_code=False,\n",
    "                                          truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".src_fin_advisor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
